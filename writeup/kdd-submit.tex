%
% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf,review=true,anonymous=true]{acmart}

\usepackage{algorithmic}
\usepackage[ruled]{algorithm2e}


%
% defining the \BibTeX command - from Oren Patashnik's original BibTeX documentation.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
   
\newcommand{\ignore}[1]{}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand{\eps}{\varepsilon}
\renewcommand{\phi}{\varphi}

   
    
% Rights management information. 
% This information is sent to you when you complete the rights form.
% These commands have SAMPLE values in them; it is your responsibility as an author to replace
% the commands and values with those provided to you when you complete the rights form.
%
% These commands are for a PROCEEDINGS abstract or paper.
\copyrightyear{2019}
\acmYear{2019}
\setcopyright{acmlicensed}
\acmConference[KDD '19]{KDD '19: SIGKDD Conference on Knowledge Discovery and Data Mining}{August 04--06, 2019}{Anchorage, AL}
\acmBooktitle{KDD '19: SIGKDD Conference on Knowledge Discovery and Data Mining, Aug 04--06, 2019, Anchorage, AL}
\acmPrice{15.00}
\acmDOI{10.1145/1122445.1122456}
\acmISBN{978-1-4503-9999-9/18/06}

%
% These commands are for a JOURNAL article.
%\setcopyright{acmcopyright}
%\acmJournal{TOG}
%\acmYear{2018}\acmVolume{37}\acmNumber{4}\acmArticle{111}\acmMonth{8}
%\acmDOI{10.1145/1122445.1122456}

%
% Submission ID. 
% Use this when submitting an article to a sponsored event. You'll receive a unique submission ID from the organizers
% of the event, and this ID should be used as the parameter to this command.
%\acmSubmissionID{123-A56-BU3}

%
% The majority of ACM publications use numbered citations and references. If you are preparing content for an event
% sponsored by ACM SIGGRAPH, you must use the "author year" style of citations and references. Uncommenting
% the next command will enable that style.
%\citestyle{acmauthoryear}

%
% end of the preamble, start of the body of the document source.
\begin{document}

%
% The "title" command has an optional parameter, allowing the author to define a "short title" to be used in page headers.
\title{New Heavy Hitters Algorithms with Tail Bounds}

%
% The "author" command and its associated commands are used to define the authors and their affiliations.
% Of note is the shared affiliation of the first two authors, and the "authornote" and "authornotemark" commands
% used to denote shared contribution to the research.
\author{Arnab Bhattacharyya}
\author{Sharanya Chakravarthi}
\author{Tejas Kaushik}
\author{Ashish Kumar Sen}
%
% By default, the full list of authors will be used in the page headers. Often, this list is too long, and will overlap
% other information printed in the page headers. This command allows the author to define a more concise list
% of authors' names for this purpose.
\renewcommand{\shortauthors}{Bhattacharyya et al.}

%
% The abstract is a short summary of the work to be presented in the article.
\begin{abstract}
In a stream of $m$ items belonging to $\{1, \dots, n\}$, the {\em $(\eps, \phi)$-Heavy Hitters} problem is to output a set $S$ of items containing all items of frequency $\geq \phi m$ and no item of frequency $< (\phi-\eps)m$. It is one of the most heavily-studied problems in data streams, with a wide variety of applications. 

In this work, we propose new randomized counter-based algorithms for the heavy-hitters problem. We show that not only do they have nearly optimal space complexity in terms of worst-case analysis, they can also be designed to have a strong error bound in terms of the frequency tail and that they perform favorably on realistic data sets.
\end{abstract}

%
% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}

%
% Keywords. The author(s) should pick words that accurately describe the work being
% presented. Separate the keywords with commas.
\keywords{data streaming, frequent items, randomized algorithms}

%
% A "teaser" image appears between the author and affiliation information and the body 
% of the document, and typically spans the page. 

%
% This command processes the author and affiliation and title information and builds
% the first part of the formatted document.
\maketitle

\section{Introduction}
The {\em data streaming} model is an important tool for analyzing algorithms on massive data sets. Streaming algorithms
take a long stream of items as input and produce a compact summary of the data as output. This summary can be used to answer queries about the properties of the input data stream. The algorithms are often (necessarily) both randomized and approximate, and we usually want them to take a single pass over the data.

A concrete application is the problem of monitoring IP network traffic. Here, data stream management systems monitor IP packets sent on communication links and perform detailed statistical analyses. These analyses are crucial for fault diagnoses and for verifying network performance and security. Examples of such systems are Gigascope at AT\&T \cite{Giga} and CMON at Sprint \cite{CMON}.The main challenge in these systems is to quickly and accurately perform the needed analyses in the face of a very high rate of updates.


In this work, we consider the {\em heavy-hitters} problem. Informally, the goal is to find the frequent elements in the stream; the same problem is also studied under the names of top-k, popular items, frequent items, elephants, or iceberg queries. It is very heavily studied and commonly used, e.g. in network analyses to find addresses that account for a large fraction of a network link utilization in a given time window. It is also often solved as a subroutine within more advanced data stream computations, e.g. approximating the entropy of a stream.

We formulate the heavy-hitters problem rigorously as follows:
\begin{definition}
 ($(\varepsilon, \varphi)$-\textbf {Heavy Hitters Problem})
    In the $(\varepsilon, \varphi)$-Heavy Hitters Problem, we are given
    parameters $0< \varepsilon <\varphi$ $\leqslant$ 1 and a stream $a_{1},....,a_{m}$
    of items $a_{j}$ in $\left\{1,2,.....n\right\}$. Let $f_{i}$ denote the number of
    occurrences of item i, i.e., its frequency. An algorithm for the problem should make one pass over the stream and at the end
    of the stream output a set $S \subseteq \left\{1,2,.....n\right\}$ which contains any $i$ such that $f_I \geq \phi m$ and does not contain any $i$ such that $f_i < (\phi - \eps)m$.
 Furthermore, for each item i $\in S$, the algorithm should output an estimate $\tilde{f_{i}}$ of the frequency $f_{i}$ which satisfies $|f_{i} -\tilde{f_{i}}|$ $ \leqslant \varepsilon m$. The algorithm may be randomized.
\end{definition}

    \section{Background}
\subsection{The \textsc{Frequent} algorithm}
    A large number of algorithms have been proposed for finding the heavy hitters and their variants. All the algorithms for finding out the heavy hitters are broadly classified into {\em Counter-based} algorithms, {\em Quantile} Algorithms,  and {\em Sketch} algorithms. 

    One of the first algorithms for the problem, as well as perhaps the most widely used, is the counter-based \textsc{Frequent} algorithm  due to Misra and Gries \cite{MG82} (independently re-discovered 20 years later with some refinements to the implementation by Karp et al. \cite{KSP03} and Demaine et al. \cite{DLM02}). It is shown below as Algorithm \ref{fig:freq}.
    
    \begin{algorithm}
    	\caption{\textsc{Frequent}}
	\label{fig:freq}
    	\SetAlgoLined
	\SetKwProg{Fn}{Function}{}{}
	
	\KwData{Number of counters $t$}

	\textbf{Initialization:}
\Begin{
	Empty set $T $\\
Empty array $c$ of length\footnote{In fact, $c$ will only contain at most $t$ nonzero elements, so it can be implemented as a table with $O(t)$ rows.} $n$\\
 }   	
	\Fn{Insert(i)}{
    	\If{$i \in T$}{
    	$c_i \leftarrow c_i + 1$;\\
	}
    	\ElseIf{$|T| < t$} {
    	 $T \leftarrow T \cup {i}$;\\
    	$c_i \leftarrow 1$;\\
}
	\Else{
    	 \ForEach{$j \in T$}{
    	$c_j \leftarrow c_j - 1$;\\
    	\lIf{$c_j = 0$}{$T \leftarrow T\setminus {j}$}
}
}
}

	\Fn{Report()}{
	\lForEach{$i \in T$}{Output $(i, c_i)$}
}
    \end{algorithm}

    \vspace*{.5cm}
 \textsc{Frequent} maintains $t$ (item, counter) pairs. Its reported frequencies are accurate to within $m/(t+1)$ where $m$ is the length of the stream. In particular, it reports all items with frequency at least $m/(t+1)$. 

In the algorithm, each new item is compared with stored items and if the new item is among the stored items, then the count value of stored items is incremented. Else, if it is not the one already present and there is some counter whose value is zero, then this item in the table is stored and the counter value is set to one. Else if all counter value is non-zero, then the counter value is decremented by 1 for each item. A simple grouping argument can be used to verify the correctness of the algorithm. More precisely, as we see below, the \textsc{Frequent} algorithm (Algorithm 1) with $t = \lceil 1/\eps \rceil$ counters solves the $(\eps,\phi)$-Heavy hitters problem using $O(\varepsilon^{-1} (\log n + \log m))$ bits of space. Note that there is no dependence on $\phi$.

We sketch an analysis of the error bound present in Berinde et al. \cite{BCIS}. 
The \textsc{Frequent} algorithm can be interpreted in the following way:
    each element in the stream results in incrementing one counter, and the insertion of some $d$ elements cause $t+1$ counters to be decremented (including the implicit decrement of the item being inserted). The sum
    of the counters at the end of the algorithm is $\norm{c}_1$. We have
   		 $\norm{c}_1 = \norm{f}_1 - d(t+1)$.
  Note that the final counter value for any element is at least  $f_i - d$. Restricting our attention to the $k$ most frequent elements\footnote{In fact, the inequality holds for {\em any} set of $k$ elements.} (which are without loss of generality, $\{1, \dots, k\}$): 
    $$\norm{c}_1 = \norm{f}_1 - d(t+1) \geq \sum_{i=1}^k(f_i-d).$$
 Therefore:
\begin{equation}
d \leq \sum_{i=k+1}^n f_i/(t-k+1)
\end{equation}
Since the error $\delta_i := f_i - c_i$ is at most $d$, by setting $k = 0$ in the above, for any $i \in [n]$:
\begin{equation}\label{eqn:std_freq}
\delta_i \leq m/(t+1)
\end{equation}
More strongly, if $F_1^{\text{res}(k)}$ is the top-$k$ residual $\sum_{i=k+1}^n f_i$, we get for any $i \in [n]$:
\begin{equation}\label{eqn:freq_tail}
\delta_i \leq F_1^{\text{res}(k)}/(t - k+1)
\end{equation}
Berinde et al.~\cite{BCIS} call a bound of the form (\ref{eqn:freq_tail}) a {\em $k$-tail guarantee}. 
Streams in real-life often display a characteristically skewed frequency distribution, with the heavy hitters
constituting a large fraction of the stream. A residual tail guarantee is in this case asymptotically better than a uniform guarantee.
As we describe later, Berinde et al.~also show that a $k$-tail guarantee implies bounds for {sparse recovery}, stronger error bounds for streams with Zipfian frequency distribution and for mergeability of multiple summaries. These properties make the \textsc{Frequent} algorithm even more useful in practice than what would be suggested just by the bound (\ref{eqn:std_freq}).

\subsection{The work of \cite{BDW16}}\label{sec:bdw}
In the recent work \cite{BDW16}, Bhattacharyya, Dey and Woodruff introduce a couple of randomized counter-based algorithms that improve upon \textsc{Frequent} in terms of the number of bits needed for storage. Their first algorithm solves the $(\eps,\phi)$-Heavy hitters problem with space $O(1/\eps \log 1/\eps + 1/\phi \log n + \log \log m)$ bits, while their second algorithm uses $O(1/\eps \log 1/\phi + 1/\phi \log n + \log \log m)$ bits. When $\eps \ll \phi$, the space requirement for these algorithms is asymptotically smaller than the worst-case space requirement for \textsc{Frequent}. In fact, \cite{BDW16} show that the second algorithm achieves the optimal (up to constant factors) number of bits for solving the $(\eps, \phi)$-Heavy hitters problem.

The first algorithm of \cite{BDW16}, which we dub \textsc{CompressedFrequent}, is shown below as Algorithm \ref{fig:alg1}.

    \begin{algorithm}
    	\caption{\textsc{CompressedFrequent}, \cite{BDW16}}
	\label{fig:alg1}
    	\SetAlgoLined
	\SetKwProg{Fn}{Function}{}{}
	
	\KwData{Parameters $\eps$ and $\phi$ with $0<\eps\leq\phi<1$.}


\ignore{
    	\textbf{Input:} A stream S of length m over universe $\mathcal{U}=[n]$; let f(x) be the frequency of $x\in \mathcal{U}$ in S\\
    	\textbf{Output:} A set $X \subseteq U$ and a function $\tilde{f} : X \rightarrow N$ such that if $f(x)\geqslant \varphi m$, then $x \in X$ and 
    	$f(x)- \varepsilon m \leqslant \tilde{f(x)} \leqslant f(x) + \varepsilon m$ and if $f(y) \leqslant \varphi m$, then y  $\notin X $  for every x, y $\in \mathcal{U}$\\
}    	
 \textbf{Initialization:}
\Begin{
$\ell \leftarrow 6\log(6/\delta)/\varepsilon^{2}$.\\ 
Hash function $h$ drawn uniformly from a universal family $H \subseteq \left\{[n] \rightarrow 
    	[4\ell^{2}/\delta ] \right\}.$ \\
An empty \textsc{Frequent} data structure\footnote{In terms of the code in Algorithm \ref{fig:freq}, $\mathcal{T}_1[i]$ is null if $i \not \in \mathcal{T}$ and equals $c_i$ otherwise.} $\mathcal{T}_{1}$ with $\varepsilon^{-1}$ counters. \\
\ignore{Each key entry of $\mathcal{T}_{1}$ can store \hspace*{.5cm} an integer of $[0,\lceil{400l^2/\delta}\rceil]$ and each value \hspace*{.6cm}entry can store an integer in $[0,11\ell]$. The \hspace*{.6cm}Table $\mathcal{T}_{1}$ will be in sorted order by value \hspace*{.6cm}throughout}
An empty set $\mathcal{T}_{2}$. \\
\ignore{Each \hspace*{.5cm}entry of $\mathcal{T}_{2}$ can store an integer in $[0,n]$. \hspace*{.5cm}The entries of $\mathcal{T}_{2}$ will correspond to ids of \hspace*{.5cm}the keys in $\mathcal{T}_{1}$ of the highest $1/\varphi$ values}
}
    
\Fn{Insert($x$)}{
 With probability $6\ell/m $, \textbf{continue}. Else, \KwRet\;
Call Insert($h(x)$) on $\mathcal{T}_1$\;
%,m     	9: Perform Misra-Gries update using $h(x)$ \hspace*{.4cm} maintaining $\mathcal{T}_{1}$ sorted by values.\\
  \If{$h(x)$ is among the  top $1/\varphi$ valued items in $\mathcal{T}_{1}$}{
\If{$x \not \in \mathcal{T}_{2}$}{
 \eIf{$|\mathcal{T}_{2}| = 1/\phi$}{
For some $y$ in $\mathcal{T}_{2}$ such that $h(y)$ is not among the top $1/\varphi$ valued items in $\mathcal{T}_{1}$, replace $y$ with $x$}
{
Insert $x$ in $\mathcal{T}_{2}$\;  
}
}
}
}
 \Fn{Report()}{
\ForEach{$x$ in $\mathcal{T}_2$}{\lIf{$\mathcal{T}_1[h(x)] \geq (\phi - \eps/2)\ell$}{Output $(x, \mathcal{T}_1[h(x)])$}}
}
    \end{algorithm}
    
\textsc{CompressedFrequent} runs Misra and Gries' \textsc{Frequent} algorithm on a sampled stream of length $O(1/\eps^2)$ on a hashed universe of size $O(1/\eps^4)$. By the choice of parameters, there are no hash collisions among the elements sampled from the stream with high probability. Also, by standard concentration results, the relative frequency of any element in the original stream and the sampled stream differ by at most $\eps/2$. Some additional bookkeeping needs to be done to ensure that we have the original (unhashed) id's of the heavy hitters, but otherwise, the analysis is complete.

The second algorithm of \cite{BDW16} is more intricate. It also operates on $O(1/\eps^2)$ samples from the original stream and then maintains a compressed histogram for hashes of the original items. It is somewhat reminiscent of the Count-Min sketch algorithm of Cormode and Muthukrishnan \cite{CM05}, but it also subsamples items from the sample with subsampling rate tuned to a mutliplicative approximation of the frequency. We omit a more detailed description of this algorithm because we do not study it in this work.

	\section{Our Contribution: Tail guarantees for randomized counter-based solutions}

We begin by observing that the randomized counter-based algorithms from \cite{BDW16}, described above in Section \ref{sec:bdw}, cannot admit a tail guarantee, like the bound in (\ref{eqn:freq_tail}) shown for \textsc{Frequent}. This is because even if the stream only has two types of elements, so that $F_1^{\text{res}(2)} = 0$, there will still be some error in the frequency approximation from the sampling step. So, random sampling cannot be used as a technique if we want to retain the tail guarantee of \textsc{Frequent}.

In this work, we show that we can improve \textsc{Frequent} in terms of bits of storage while still achieving the $k$-tail guarantee. We show two different algorithms with this feature. Although they are novel to the best of our knowledge, they are both quite simple to implement and analyze. The first algorithm combines a \textsc{Frequent} data structure with a Count-Min sketch \cite{CM05}. The second algorithm maintains two \textsc{Frequent} data structures, one with a smaller number of counters containing items from the original universe and another with a larger number of counters recording hashes of the original item id's.

Armed with the tail guarantee and following the analysis of Berinde et al.~\cite{BCIS}, we can then show that our proposed algorithms enjoy several other appealing properties, such as guarantees for skewed Zipfian streams, for the sparse recovery problem and for merging multiple streams.

\section{New Algorithms}

\subsection{\textsc{SketchFrequent}}
 	\begin{algorithm}
		\caption{\textsc{SketchFrequent}}
    	\SetAlgoLined
	\SetKwProg{Fn}{Function}{}{}

	\KwData{Parameters $\eps, \phi, \delta \in (0,1)$ with $\eps\leq\phi$.}


\textbf{Initialization}:
\Begin{
$\ell \leftarrow 2\log(1/\delta\phi)$\\
$w \leftarrow 2/\eps$\\
Hash functions $h_1, \dots, h_\ell$ drawn uniformly from a universal family $H \subseteq \left\{[n] \to [w] \right\}.$ \\
Empty \textsc{Frequent} data structure $\mathcal{T}$ with $2/\phi$ counters.\\
Empty matrix $\mathcal{C}$ with $\ell$ rows and $w$ columns.
}
\Fn{Insert($x$)}{
Insert $x$ into $\mathcal{T}$\;
\For{$i = 1$ to $\ell$}
{Increment $\mathcal{C}[i][h_i(x)]$\;}
}
\Fn{Report()}{
\ForEach{$x \in \mathcal{T}_{1}$}{
$\hat{f}_x \leftarrow \min_{i \in [\ell]} \mathcal{C}[i][h_i(x)]$\;
\If{$\hat{f}(x) \geq \phi m$}{
Output $(x,\hat{f}_x)$\;
}}}
	\end{algorithm}
Our first algorithm, shown above, runs the Count-Min sketch on the output of a \textsc{Frequent} counter solving the $(\phi,\phi/2)$-Heavy hitters problem.

\begin{theorem}
\textsc{SketchFrequent} solves the $(\phi, \eps)$-Heavy hitters problem. In particular, for any $0 \leq k \leq \sqrt{\delta w/\ell}$, with probability at least $1-\delta$, for any $(x, \hat{f}_x)$ reported by the algorithm,
$$0 \leq \hat{f}_x - f_x \leq \eps F_1^{\text{res}(k)}.$$
The algorithm uses $O(\phi^{-1} \log  n + \eps^{-1} \log \phi^{-1} \log m)$ bits of storage.
\end{theorem}
\begin{proof}
Correctness and the space bound follows from the guarantees for the Count-Min sketch and taking the union bound on the $2/\phi$ elements in $\mathcal{T}$ (see, for example, \cite{ChekuriNotes}). The tail bound arises from the fact that on the top $\sqrt{\delta w/\ell}$ elements of the stream, $h_1, \dots, h_\ell$ do not collide with probability at least $1-\delta$ (similar to Lemma \ref{lem:nocoll} below). 
\end{proof}

\subsection{\textsc{DoubleFrequent}}

 	\begin{algorithm}
		\caption{\textsc{DoubleFrequent}}
    	\SetAlgoLined
	\SetKwProg{Fn}{Function}{}{}

	\KwData{Parameters $\eps, \phi, \delta \in (0,1)$ with $\eps\leq\phi/2$.}


\ignore{		\textbf{Input:} A stream S of length m over universe $\mathcal{U}=[n]$; let f(x) be the frequency of $x\in \mathcal{U}$ in S\\
		\textbf{Output:} A set $X \subseteq U$ and a function $\tilde{f} : X \rightarrow N$ such that if $f(x)\geqslant \varphi m$, then $x \in X$ and 
		$f(x)- \varepsilon m \leqslant \tilde{f(x)} \leqslant f(x) + \varepsilon m$ and if $f(y) \leqslant \varphi m$, then y  $\notin X $  for every x, y $\in \mathcal{U}$\\
}	
\textbf{Initialization}:
\Begin{
Hash function $h$ drawn uniformly from a universal family $H \subseteq \left\{[n] \to [4/(\delta\eps^2)] \right\}.$ \\
An empty \textsc{Frequent} data structure $\mathcal{T}_{1}$ with $2/\phi$ counters. \\
An empty \textsc{Frequent} data structure  $\mathcal{T}_{2}$ with $1/\eps$ counters. \\
}
\Fn{Insert($x$)}{
Insert $x$ into $\mathcal{T}_{1}$\;
Insert $h(x)$ into $\mathcal{T}_{2}$\;
}
\Fn{Report()}{
\ForEach{$x \in \mathcal{T}_{1}$}{
\If{$\mathcal{T}_1(x) \geq \frac{\phi m}{2} \wedge \mathcal{T}_{2}[h(x)]\geq (\varphi-{\varepsilon}{})m$}{
Output $(x,\mathcal{T}_{2}[h(x)])$\;
}}}
	\end{algorithm}

	\vspace*{.3cm}

Our second algorithm runs two \textsc{Frequent} algorithms in parallel. The first is a gross estimation of the frequencies with relative error up to $\phi/2$. This is done to filter out a list of $O(1/\phi)$ elements on which we would like the estimate to be more accurate. For this, we use more counters but we also hash the universe size down to $\textrm{poly}(1/\eps)$ to save space. The resulting algorithm inherits the desirable tail behavior of \textsc{Frequent}.

\ignore{
	Proposed algorithm is simple algorithm with space complexity of $O(1/\varphi\log n + 1/\varepsilon\log {1/\varepsilon} + 1/\varepsilon \log s)$. In this proposed algorithm, upon picking item $x$ from stream S, If $x$ is already present in $\mathcal{T}_{1}$ then increment its counter value by 1.If $x$ is not present in $\mathcal{T}_{1}$ and $\mathcal{T}_{1}$ is not Full then put $x$ into $\mathcal{T}_{1}$ and make its counter value 1. If $x$ is not present in $\mathcal{T}_{1}$ and $\mathcal{T}_{1}$ is Full then decrement value for each counter value in $\mathcal{T}_{1}$ and if any counter value of some element say $y$ is zero then replace $y$ with $x$ and make $x$ counter value to be 1.
}
\begin{theorem}
\textsc{DoubleFrequent} solves the $(\phi, 2\eps)$-Heavy hitters problem. In particular, for any $0\leq k \leq 1/\eps$, with probability at least $1-\delta$, for any $(x,\hat{f}_x)$ reported by the algorithm,
$$|f_x - \hat{f}_x| \leq \frac{F_1^{\text{res}(k)}}{1/\eps - k}$$
The algorithm uses $O(\phi^{-1} \log n + \eps^{-1} \log m)$ bits of storage.
\end{theorem}
\begin{proof}
First, observe that there are likely going to be no hash collisions among the top $1/\eps$ elements in the stream.	
\begin{lemma}\label{lem:nocoll}
If $S \subseteq [n]$ of size $1/\varepsilon$ and $\mathcal{H} \subseteq \{h|h:[n]\to [4 \delta^{-1}\varepsilon^{-2}]\}$ is a universal hash family, then:
$$\Pr[\exists i \neq j \in S, h(i) = h(j)] \leq \delta/4$$.
 where $h$ is chosen from $\mathcal{H}$ uniformly at random.\\
\end{lemma}
\begin{proof}
By the union bound and definition of universal hash family,
\begin{align*}
\Pr[\exists i \neq j \in S, h(i) = h(j)] &\leq \sum_{i\neq j \in S} \Pr[h(i)=h(j)]\\ 
&\leq \frac{1}{\eps^2} \frac{\delta \eps^2}{4} = \frac{\delta}{4}
\end{align*}
\end{proof}
Henceforth, condition on Lemma \ref{lem:nocoll} holding true for the top $1/\eps$ elements. Define:
$$\overline{f}_x=\sum_{y:h(y)=h{x}}f_y.$$
Clearly, $\overline{f}_x \geq f_x$ for any $x$.
Also, without loss of generality, suppose that $f_1 \geq f_2 \geq \cdots \geq f_n$ so that:
$$F^{\text{res}(k)}_1=\sum_{i=k+1}^n f_{i}.$$
\begin{lemma}
For any $0 \leq k \leq 1/\eps$:
$$\Pr[\exists x, \mathcal{T}_1[x] \geq \phi m/2 \wedge \overline{f}_x > f_x +\frac{\eps}{2} F_1^{\text{res}(k)}] < \frac{\delta}{2}$$
\end{lemma}
\begin{proof}
We observe that for $x$ is among the top $2/\phi \leq 1/\eps$ elements. For any $x<1/\eps$:
\begin{equation*}
\mathbb{E} \overline{f}_x = f_x + \mathbb{E} \sum_{y \neq x: h(y)=h(x)} f_y
 = f_x + \mathbb{E} \sum_{y > 1/\eps: h(y)=h(x)} f_y
\end{equation*}
where the last equality is from conditioning on no collisions in the top $1/\eps$ elements. Again, using the definition of universal hash family, we get that:
$$\mathbb{E} \overline{f}_x \leq f_x + \frac{\delta \eps^2}{4} F_1^{\text{res}(k)}$$
for any $k \leq 1/\eps$. By the union bound:
$$\Pr[\exists x \leq 1/\eps, \overline{f}_x > f_x + \eps F_1^{\text{res}(k)}]< \frac{1}{\eps} \frac{\delta\eps}{2} = \frac{\delta}{2}$$
\end{proof}
We now bound the error from the use of the \textsc{Frequent} algorithm in $\mathcal{T}_2$.
\begin{lemma}
For all $x$ and all $0 \leq k \leq 1/\eps$:
$$\overline{f}_x - \mathcal{T}_2[h(x)] \leq \frac{F_1^{\text{res}(k)}}{1/\eps - k}$$
\end{lemma}
\begin{proof}
This directly follows from the bound (\ref{eqn:freq_tail}) shown in \cite{BICS} and the fact that the top-$k$ residual of the $\overline{f}_x$ is at most the top-$k$ residual of $f_x$.
\end{proof}
Putting the lemmas together, we get that with probability at least $1-\delta$, for all $x$ such that $\mathcal{T}_1[x] \geq \phi m/2$:
\begin{align*}
f_x \leq \overline{f}_x \leq \mathcal{T}_2[h(x)] + \frac{F_1^{\text{res}(k)}}{1/\eps -k}
\end{align*}
and
\begin{align*}
f_x \geq \overline{f}_x - \frac{\eps}{2} F_1^{\text{res}(k)} \geq \mathcal{T}_2[h(x)] - \frac{\eps}{2} F_1^{\text{res}(k)} 
\end{align*}
The error bound follows as $\eps/2 < 1/(1/\eps - k)$ for any $k \geq 0$.

The space bound for \textsc{DoubleFrequent} is immediate from the space bound for \textsc{Frequent}.
\end{proof}
\ignore{
	  Assumption $1/\varepsilon>1/\varepsilon $
	  Without loss of generality suppose\\
	   $f_1 \geq f_2 \geq.......\geq f_n$\\
	   \\
	   \hspace*{3cm}$$F^{res(k)}_1=\sum_{i=k+1}^n f_{i}$$\\
	   \\
	   For any $x$ in $\mathcal{T}_{1}$, let $\delta_x=|f_x-T[h(x)]|$\\
	   
	  Lemma 2: $\forall x$ in $\mathcal{T}_{1}$ and $0\leq k < 1/\varepsilon$,\\
	  \\
	 \hspace*{1cm} $E\delta_x \leq \frac{\varepsilon^2}{100}F^{res(k)} + \frac{F^{res(k)}}{1/\varepsilon-k}$\\
	 
	 Proof for Lemma2:
	
	let $$\overline{f_x}=\sum_{y:h(y)=h{x}}f_y$$
	
	claim 1: $$E\overline{f_x}-f_x \leq \frac{\varepsilon^2}{100}F^{res(1/\varepsilon)}$$\\
	\\
	Proof for claim1: $$\overline{f_x}= f_x +\sum_{y\neq x:h(y)=h{x}}f_y$$
	We are assuming that among the first $1/\epsilon$ heaviest elements, there is no collision, Collision are coming only outside the $1/\varepsilon$ elements\\
	 $$\overline{f_x}= f_x +\sum_{y>1/\varepsilon:h(y)=h{x}}f_y$$
	 
	 take expectation on  both side and we get,\\
	  $$E\overline{f_x}=f_x+E(\sum_{y>1/\varepsilon:h(y)=h(x)}f_y)$$	
	  $$E\overline{f_x}=f_x+E(\sum_{y>1/\varepsilon}f_y 1\{h(y)=h(x)\})$$
	  $$E\overline{f_x}=f_x+\sum_{y>1/\varepsilon}f_y .E[1\{h(y)=h(x)\}]$$
	  $$E\overline{f_x}=f_x+\sum_{y>1/\varepsilon}f_y. p(1\{h(y)=h(x)\})$$	
	  since by the property of universal hash function: $$p(1\{h(y)=h(x)\})=\frac{1}{100/\varepsilon^2}$$
	  $$E\overline{f_x}=f_x+\sum_{y>1/\varepsilon}f_y. \frac{1}{100/\varepsilon^2}$$
	  $$E\overline{f_x}=f_x+\sum_{y>1/\varepsilon}f_y. \frac{\varepsilon^2}{100}$$
	  $$E\overline{f_x}=f_x+\frac{\varepsilon^2}{100}\sum_{y>1/\varepsilon}f_y$$
	  $$E\overline{f_x}=f_x+\frac{\varepsilon^2}{100}F^{res(1/\varepsilon)}$$...$(2)$
	  \\
	  From [BCIS]
	   $$\overline{f_x}-\mathcal{T}_{2}[h(x)] \leq \frac {\overline{F}^{res(k)}}{1/\varepsilon-k} \leq \frac{F^{res(k)}}{1/\varepsilon-k}$$ ........$(1)$ 	  
	                \\                            
	   where $$\overline{F}^{res(k)}=\sum_{y:h(y)\neq h(1)...h(k)}f_y$$\\
	\\
	   By combining equation 1 and 2, We get\\
	   \\
	   $$E\mathcal{T}_{2}[h(x)]-f_x \leq \frac{\varepsilon^2}{100}F^{res(1/\varepsilon)}+\frac{F^{res(k)}}{1/\varepsilon-k}$$
	}

\subsection{Consequences}
Berinde et al. \cite{BCIS} show that the tail guarantee implies desirable behavior for a few other settings. We
state the results below for the \textsc{DoubleFrequent} algorithm, though similar results should also hold for
the \textsc{SketchFrequent} algorithm.

\subsubsection{Zipfian distribution}
A stream of length $m$ on a universe of size $n$ is said
be {\em Zipfian} if there exists $\alpha \geq 1$ such that for every
$i \in [n]$,
$$f_i =\frac{m}{i^\alpha \zeta(\alpha)} \quad \text{ where }	 \quad \zeta(\alpha)=\sum_{i=1}^n \frac{1}{i^\alpha}.$$

In fact, for the results here, it is only required that
the tail of the distribution be upper-bounded by the
above Zipfian distribution. Of course, the order of
arrivals in the stream is arbitrary.
\begin{theorem}Given a Zipfian stream with parameter
$\alpha\geq 1$, the error in the frequency for any item reported by the \textsc{DoubleFrequent} algorithm with parameters $\delta, \eps,$ and $\varphi$ is at most 
$O(\eps^\alpha m)$ with probability at least $1 -\delta$.
\end{theorem}
\ignore{
Proof. Corollary of Theorem 8 in [6].
5
4.3.2 k-sparse recovery
In the k-sparse recovery problem, given a frequency
vector f, we want to find a vector f
0
such that f
0
is
k-sparse (has only k nonzero entries) and the p-norm
error kf − f
0kp = (P
i
|fi − f
0
i
|
p
)
1/p is minimized.
The next theorem shows that if k ≤ 1/ϕ, taking the
top k of the output of DoubleFrequent is a good
approximation.
Theorem 4. If k ≤ 1/ϕ and f
0
is the k-sparse vector obtained by taking the top k elements of the output
of DoubleFrequent (run with parameters δ, ε and
ϕ), with probability at least 1 − δ, for any p ≥ 1:
kf − f
0
kp ≤ (F
res(k)
p
)
1/p + O

εFres(k)
1
k
2−1/p !
where (F
res(k)
p )
1/p is the smallest Lp error of any ksparse recovery of f.
Proof. Corollary of Theorem 5 in [6].
The following result also gives a guarantee if we
would like to output exactly the top k elements.
Theorem 5. Given a Zipfian stream with parameter
α > 1, if k ≤ O((α/ϕα)
1/(α+1)), then with probability at least 1−δ, DoubleFrequent can retrieve the
top-k elements of the stream in correct order.
Proof. Corollary of Theorem 9 of [6].
}

%
% The next two lines define the bibliography style to be used, and the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{papers}

% 
% If your work has an appendix, this is the place to put it.
\end{document}
